{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.mpe import simple_tag_v3\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取参数并创建环境\n",
    "args = get_args()\n",
    "env = simple_tag_v3.env(render_mode='human',max_cycles=10000,continuous_actions=True)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:59: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "  \"The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\"\n",
      "d:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:73: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  \"The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\"\n"
     ]
    }
   ],
   "source": [
    "# env.agents\n",
    "# ['adversary_0', 'adversary_1', 'adversary_2', 'agent_0']\n",
    "# n_player：总的智能体个数\n",
    "# n_agents：adv智能体个数\n",
    "# obs_shape：adv智能体观测空间维度列表\n",
    "# action_shape：adv智能体动作空间维度列表\n",
    "args.n_players = len(env.agents)\n",
    "args.n_agents = args.n_players-1\n",
    "args.obs_shape = []\n",
    "args.action_shape=[]\n",
    "for i in range(args.n_agents):  \n",
    "    cur_agent=env.agents[i]\n",
    "    args.obs_shape.append(env.observation_spaces[cur_agent].shape[0])\n",
    "\n",
    "    # 动作空间为离散空间时\n",
    "    # args.action_shape.append(env.action_spaces[cur_agent].n)\n",
    "\n",
    "    args.action_shape.append(env.action_spaces[cur_agent].shape[0])\n",
    "    \n",
    "args.high_action = 1\n",
    "args.low_action = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 0 successfully loaded actor_network: ./model/simple_tag/agent_0/bst_actor_params.pkl\n",
      "Agent 0 successfully loaded critic_network: ./model/simple_tag/agent_0/bst_critic_params.pkl\n",
      "Agent 1 successfully loaded actor_network: ./model/simple_tag/agent_1/bst_actor_params.pkl\n",
      "Agent 1 successfully loaded critic_network: ./model/simple_tag/agent_1/bst_critic_params.pkl\n",
      "Agent 2 successfully loaded actor_network: ./model/simple_tag/agent_2/bst_actor_params.pkl\n",
      "Agent 2 successfully loaded critic_network: ./model/simple_tag/agent_2/bst_critic_params.pkl\n",
      "agents创建完毕!\n"
     ]
    }
   ],
   "source": [
    "# 智能体更新\n",
    "agents = []\n",
    "for i in range(args.n_agents):\n",
    "    agent = Agent(i, args)\n",
    "    agents.append(agent)\n",
    "    agent.policy.load_model()\n",
    "print(\"agents创建完毕!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_episodes=10\n",
    "evaluate_episodes_len=100\n",
    "# 回报列表，用于进行reward均值图像的绘制\n",
    "return_list=[]\n",
    "\n",
    "# 创建一个经验缓冲区，用于经验回放\n",
    "buffer=Buffer(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]d:\\project_code\\python_project\\Public_Project\\MAEnv\\mpeEnvs\\utils.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  transitions[key] = torch.tensor(transitions[key], dtype=torch.float32).to(device)\n",
      " 10%|█         | 1/10 [00:03<00:35,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:07<00:30,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:11<00:26,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:15<00:22,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:18<00:18,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:22<00:15,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:27<00:12,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:31<00:08,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:35<00:04,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:40<00:00,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_episodes=10\n",
    "train_episodes_len=100\n",
    "\n",
    "return_list=[]\n",
    "for episode in tqdm(range(train_episodes)):\n",
    "    env.reset()\n",
    "    rws=0\n",
    "    for i in range(train_episodes_len):\n",
    "        u = []\n",
    "        actions = []\n",
    "        with torch.no_grad():\n",
    "            for agent_id, agent in enumerate(agents):\n",
    "                # x \\in [-1,1]转换为y \\in [0,1]，变化的公式为：y=(x+1)/2,x=y*2-1\n",
    "                action=agent.select_action(env.observe(env.agents[agent_id]),args.noise_rate,args.epsilon)\n",
    "                action=(action+1)/2\n",
    "                actions.append(action)\n",
    "                u.append(action)\n",
    "\n",
    "        for i in range(args.n_agents, args.n_players):\n",
    "            actions.append([0, np.random.rand() , 0, np.random.rand() , 0])\n",
    "\n",
    "        s_next,r=[],[]\n",
    "        s=[]\n",
    "        idx=0\n",
    "            \n",
    "        # 每次迭代的时候需要将智能体的动作列表一一送入进行遍历，并将每个智能体的对应数据记录\n",
    "        for agent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "            \n",
    "            s.append(env.observe(env.agents[idx]))\n",
    "            env.step(np.array(actions[idx]).astype(np.float32))\n",
    "            s_next.append(env.observe(env.agents[idx]))\n",
    "\n",
    "            r.append(reward)\n",
    "\n",
    "            idx+=1\n",
    "            if idx>3: break\n",
    "        \n",
    "        buffer.store_episode(s[:args.n_agents], u, r[:args.n_agents], s_next[:args.n_agents])\n",
    "        \n",
    "        s = s_next\n",
    "        if buffer.current_size >= args.batch_size:\n",
    "            transitions = buffer.sample(args.batch_size)\n",
    "            for agent in agents:\n",
    "                other_agents = agents.copy()\n",
    "                other_agents.remove(agent)\n",
    "                agent.learn(transitions, other_agents)\n",
    "        \n",
    "        rws+=r[0]\n",
    "\n",
    "        args.noise = max(0.05, args.noise_rate - 0.0000005)\n",
    "        args.epsilon = max(0.05, args.epsilon - 0.0000005)\n",
    "    print(rws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "5.0\n",
      "5.0\n",
      "-1541.4595537035675\n"
     ]
    }
   ],
   "source": [
    "return_list=[]\n",
    "for episode in range(evaluate_episodes):\n",
    "    # reset the environment\n",
    "    env.reset()\n",
    "    \n",
    "    rewards=[0 for i in range(args.n_players)]\n",
    "    done = False\n",
    "    for i in range(evaluate_episodes_len):\n",
    "        # env.render()\n",
    "        # 单步步长移动对应的代码\n",
    "        actions=[]\n",
    "\n",
    "        # 为每个智能体确定动作\n",
    "        with torch.no_grad():\n",
    "            for agent_id,agent in enumerate(agents):\n",
    "                action=agent.select_action(env.observe(env.agents[agent_id]),args.noise_rate,args.epsilon)\n",
    "                action=(action+1)/2\n",
    "                actions.append(action)\n",
    "\n",
    "        # 为每个非智能体确定动作\n",
    "        for i in range(args.n_agents,args.n_players):\n",
    "            # 非智能体仅通过随机移动改变状态\n",
    "            actions.append([0,np.random.rand(),0,np.random.rand(),0])\n",
    "\n",
    "\n",
    "        s_next,r=[],[]\n",
    "        idx=0\n",
    "        \n",
    "        # 每次迭代的时候需要将智能体的动作列表一一送入进行遍历，并将每个智能体的对应数据记录\n",
    "        for agent in env.agent_iter():\n",
    "            observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            env.step(np.array(actions[idx]).astype(np.float32))\n",
    "\n",
    "            s_next.append(env.observe(env.agents[idx]))\n",
    "            r.append(reward)\n",
    "\n",
    "            idx+=1\n",
    "            if idx>3:\n",
    "                break\n",
    "\n",
    "        # 第0个adv的奖励总和\n",
    "        # 每轮的reward\n",
    "        rewards=[r[i]+rewards[i] for i in range(len(rewards))]\n",
    "\n",
    "    return_list.append(rewards)\n",
    "\n",
    "return_list=np.array(return_list)\n",
    "\n",
    "for i in range(len(return_list[0])):\n",
    "    print(sum(return_list[:,i])/evaluate_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
