
## np、torch等库函数相关
```python
# 可以生成从第一个参数到第二个参数数值-1的一个线性array
>>> a=np.arange(10,100)
>>> a
array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,
       27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,
       44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,
       61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,
       78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94,
       95, 96, 97, 98, 99])

# 创建一个randint随机数，上下界为前两个参数，生成的数目为第三个参数
>>> b=np.random.randint(0,10,100)
>>> b
array([2, 2, 3, 1, 3, 9, 0, 8, 1, 7, 4, 2, 1, 0, 7, 4, 2, 1, 1, 3, 8, 5,
       2, 9, 8, 8, 2, 9, 7, 4, 6, 8, 5, 1, 2, 2, 2, 3, 8, 3, 6, 8, 5, 7,
       7, 8, 8, 7, 1, 2, 0, 5, 8, 6, 6, 9, 6, 2, 7, 3, 8, 1, 1, 2, 4, 7,
       9, 3, 1, 6, 3, 1, 7, 9, 9, 8, 9, 6, 8, 3, 7, 9, 0, 9, 4, 5, 0, 8,
       4, 7, 8, 2, 2, 4, 7, 4, 8, 0, 3, 1])

# 类似于列表的append函数
>>> np.concatenate([a,b])
array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,
       27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,
       44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,
       61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,
       78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94,
       95, 96, 97, 98, 99,  6])

# 进行相应维度的拼接
>>> import torch
>>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])
>>> y = torch.tensor([[7, 8, 9], [10, 11, 12]])
>>> torch.cat((x, y), dim=0)
tensor([[ 1,  2,  3],
        [ 4,  5,  6],
        [ 7,  8,  9],
        [10, 11, 12]])
>>> torch.cat((x, y), dim=1)
tensor([[ 1,  2,  3,  7,  8,  9],
        [ 4,  5,  6, 10, 11, 12]])
```

```python
# inspect.getfullargspec是Python标准库中的一个函数，用于获取函数的参数信息。它返回一个命名元组，其中包含函数的参数名称、默认值、注解和其他相关信息。

inspect.getfullargspec(func)

# 参数说明：
# func：要获取参数信息的函数对象。
# 返回值是一个命名元组，包含以下属性：
# args：一个列表，包含函数的位置参数名称。
# varargs：一个字符串，表示函数的可变位置参数名称（如果有）。
# varkw：一个字符串，表示函数的可变关键字参数名称（如果有）。
# defaults：一个元组，包含函数的默认参数值（如果有）。
# kwonlyargs：一个列表，包含函数的仅关键字参数名称（Python 3.x中引入）。
# kwonlydefaults：一个字典，包含函数的仅关键字参数的默认值（Python 3.x中引入）。
# annotations：一个字典，包含函数参数的注解（Python 3.x中引入）。
```

## gym环境及强化学习相关

```python
env.observation_space[i].shape[0]:
# 其中env.observation_space[i]会返回当前智能体对应的观测空间的类别，例如Box(16,),注重的应该是维度，而不是数值
# .shape会返回(16,)元组，.shape[0]则会返回16，刚好对应维度

# 策略网络
# 也叫做Actor网络，输入为obs维度的输入，输出为action维度的输出，在MADDPG中每个不同的agent都有一个自己的actor网络
# 最终通过最大的动作数值作乘积，来令tanh函数归一化操作后的数值范围在最大与最小的action范围内合理分布
class Actor(nn.Module):
    def __init__(self, args, agent_id):
        super(Actor, self).__init__()
        self.max_action = args.high_action
        self.fc1 = nn.Linear(args.obs_shape[agent_id], 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 64)
        self.action_out = nn.Linear(64, args.action_shape[agent_id])

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        actions = self.max_action * torch.tanh(self.action_out(x))
        return actions


# 价值网络
# 用于估计Q函数的具体数值，输入维度为所有的观测以及动作维度的和，输出为1维，数值上表示为环境的Q函数
# 前向传播：
# 将每个智能体的状态拼接到一起，同时再将每个智能体的动作缩放至0~1之间，然后跟状态一样拼接，最后将动作与状态拼接，作为总体的输入
class Critic(nn.Module):
    def __init__(self, args):
        super(Critic, self).__init__()
        self.max_action = args.high_action
        self.fc1 = nn.Linear(sum(args.obs_shape) + sum(args.action_shape), 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 64)
        self.q_out = nn.Linear(64, 1)

    def forward(self, state, action):
        state = torch.cat(state, dim=1)
        for i in range(len(action)):
            action[i] /= self.max_action
        action = torch.cat(action, dim=1)
        x = torch.cat([state, action], dim=1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        q_value = self.q_out(x)
        return q_value

# MADDPG总体设计
# actor网络+actor目标网络，critic+critic目标网络，目标网络的参数均初始化为最初的actor与critic网络参数
# MADDPG:
# self.actor self.actor_target self.critic self.critic_target
class MADDPG:
    def __init__(self, args, agent_id):  # 因为不同的agent的obs、act维度可能不一样，所以神经网络不同,需要agent_id来区分
        self.args = args
        self.agent_id = agent_id
        self.train_step = 0

        # create the network
        self.actor_network = Actor(args, agent_id)
        self.critic_network = Critic(args)

        # build up the target network
        self.actor_target_network = Actor(args, agent_id)
        self.critic_target_network = Critic(args)

        # load the weights into the target networks
        self.actor_target_network.load_state_dict(self.actor_network.state_dict())
        self.critic_target_network.load_state_dict(self.critic_network.state_dict())

        # create the optimizer
        self.actor_optim = torch.optim.Adam(self.actor_network.parameters(), lr=self.args.lr_actor)
        self.critic_optim = torch.optim.Adam(self.critic_network.parameters(), lr=self.args.lr_critic)

        # create the dict for store the model
        if not os.path.exists(self.args.save_dir):
            os.mkdir(self.args.save_dir)
        # path to save the model
        self.model_path = self.args.save_dir + '/' + self.args.scenario_name
        if not os.path.exists(self.model_path):
            os.mkdir(self.model_path)
        self.model_path = self.model_path + '/' + 'agent_%d' % agent_id
        if not os.path.exists(self.model_path):
            os.mkdir(self.model_path)

        # 加载模型
        if os.path.exists(self.model_path + '/actor_params.pkl'):
            self.actor_network.load_state_dict(torch.load(self.model_path + '/actor_params.pkl'))
            self.critic_network.load_state_dict(torch.load(self.model_path + '/critic_params.pkl'))
            print('Agent {} successfully loaded actor_network: {}'.format(self.agent_id,
                                                                          self.model_path + '/actor_params.pkl'))
            print('Agent {} successfully loaded critic_network: {}'.format(self.agent_id,
                                                                           self.model_path + '/critic_params.pkl'))

    # soft update
    def _soft_update_target_network(self):
        for target_param, param in zip(self.actor_target_network.parameters(), self.actor_network.parameters()):
            target_param.data.copy_((1 - self.args.tau) * target_param.data + self.args.tau * param.data)

        for target_param, param in zip(self.critic_target_network.parameters(), self.critic_network.parameters()):
            target_param.data.copy_((1 - self.args.tau) * target_param.data + self.args.tau * param.data)

    # update the network
    def train(self, transitions, other_agents):
        for key in transitions.keys():
            transitions[key] = torch.tensor(transitions[key], dtype=torch.float32)
        r = transitions['r_%d' % self.agent_id]  # 训练时只需要自己的reward
        o, u, o_next = [], [], []  # 用来装每个agent经验中的各项
        for agent_id in range(self.args.n_agents):
            o.append(transitions['o_%d' % agent_id])
            u.append(transitions['u_%d' % agent_id])
            o_next.append(transitions['o_next_%d' % agent_id])

        # calculate the target Q value function
        u_next = []
        with torch.no_grad():
            # 得到下一个状态对应的动作
            index = 0
            for agent_id in range(self.args.n_agents):
                if agent_id == self.agent_id:
                    u_next.append(self.actor_target_network(o_next[agent_id]))
                else:
                    # 因为传入的other_agents要比总数少一个，可能中间某个agent是当前agent，不能遍历去选择动作
                    u_next.append(other_agents[index].policy.actor_target_network(o_next[agent_id]))
                    index += 1
            q_next = self.critic_target_network(o_next, u_next).detach()

            target_q = (r.unsqueeze(1) + self.args.gamma * q_next).detach()

        # the q loss
        q_value = self.critic_network(o, u)
        critic_loss = (target_q - q_value).pow(2).mean()

        # the actor loss
        # 重新选择联合动作中当前agent的动作，其他agent的动作不变
        u[self.agent_id] = self.actor_network(o[self.agent_id])
        actor_loss = - self.critic_network(o, u).mean()
        # if self.agent_id == 0:
        #     print('critic_loss is {}, actor_loss is {}'.format(critic_loss, actor_loss))
        # update the network
        self.actor_optim.zero_grad()
        actor_loss.backward()
        self.actor_optim.step()
        self.critic_optim.zero_grad()
        critic_loss.backward()
        self.critic_optim.step()

        self._soft_update_target_network()
        if self.train_step > 0 and self.train_step % self.args.save_rate == 0:
            self.save_model(self.train_step)
        self.train_step += 1

    def save_model(self, train_step):
        num = str(train_step // self.args.save_rate)
        model_path = os.path.join(self.args.save_dir, self.args.scenario_name)
        if not os.path.exists(model_path):
            os.makedirs(model_path)
        model_path = os.path.join(model_path, 'agent_%d' % self.agent_id)
        if not os.path.exists(model_path):
            os.makedirs(model_path)
        torch.save(self.actor_network.state_dict(), model_path + '/' + num + '_actor_params.pkl')
        torch.save(self.critic_network.state_dict(),  model_path + '/' + num + '_critic_params.pkl')



```

## github上传/更新代码

```git
<!-- 简单记录方便进行复制 -->
git add .
git commit -m "提交更新的代码"
git pull origin branch名字
git push origin branch名字
```